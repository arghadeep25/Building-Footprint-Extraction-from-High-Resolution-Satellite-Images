{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inria_Building_Dataset_Segmentation_with_FCN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZzA9epqzSE4",
        "colab_type": "text"
      },
      "source": [
        "#Load from ZIP file#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DLx9lS9yuYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip dummy_Inria_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az4uL6raxgGD",
        "colab_type": "text"
      },
      "source": [
        "#Define Training Path#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9nAWg8bxigH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TRAIN_PATH = '/content/gdrive/My Drive/thesis_data/inria_dataset_256/train/'\n",
        "TRAIN_PATH = 'dummy_Inria_data/train/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj-2OHyVzf2r",
        "colab_type": "text"
      },
      "source": [
        "#FCN#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inqIhNyqWq9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import time\n",
        "import random\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as keras\n",
        "from skimage.transform import resize\n",
        "from keras.utils import plot_model, Sequence\n",
        "import keras.backend.tensorflow_backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.layers import Conv2D, Input, MaxPool2D, UpSampling2D, Concatenate\n",
        "from skimage.transform import resize, rotate, rescale, warp, AffineTransform\n",
        "\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D, Deconvolution2D, Cropping2D\n",
        "from keras.layers import Input, Add, Dropout, Permute, add"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLUmDDhgWvqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataAugmentation():\n",
        "    \"\"\" Class for data augmentation.\n",
        "\n",
        "        Mainly helps when the training data is small\n",
        "\n",
        "        Parameters: image, mask,\n",
        "                    rotation angle (default = 90)\n",
        "                    zoom_range (default = 1)\n",
        "                    horizontal_flip (default = False)\n",
        "                    vertical_flip (default = False)\n",
        "                    activate (default = False)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image, mask,\n",
        "                rotation = 0,\n",
        "                zoom_range = 1,\n",
        "                horizontal_flip = False,\n",
        "                vertical_flip = False,\n",
        "                shear = 0,\n",
        "                activate = False):\n",
        "\n",
        "        self.image = image\n",
        "        self.mask = mask\n",
        "        self.rotation = rotation\n",
        "        self.zoom_range = zoom_range\n",
        "        self.horizontal_flip = horizontal_flip\n",
        "        self.vertical_flip = vertical_flip\n",
        "        self.shear = shear\n",
        "        self.activate = activate\n",
        "\n",
        "    def rotate_data(self):\n",
        "        \"\"\" Rotation\n",
        "        \"\"\"\n",
        "        self.image = rotate(self.image, self.rotation)\n",
        "        self.mask = rotate(self.mask, self.rotation)\n",
        "        return self.image, self.mask\n",
        "\n",
        "    def rescale_data(self):\n",
        "        \"\"\" Rescaling\n",
        "        \"\"\"\n",
        "        height, width, _ = self.image.shape\n",
        "        self.image = rescale(self.image, self.zoom_range)\n",
        "        self.image = resize(self.image, (height, width))\n",
        "\n",
        "        self.mask = rescale(self.mask, self.zoom_range)\n",
        "        self.mask = resize(self.mask, (height, width))\n",
        "        return self.image, self.mask\n",
        "\n",
        "    def flip_horizontal_data(self):\n",
        "        \"\"\" Flip Horizontally\n",
        "        \"\"\"\n",
        "        if self.horizontal_flip == True:\n",
        "            flipped_image = np.flip(self.image, 1)\n",
        "            flipped_mask = np.flip(self.mask, 1)\n",
        "            return flipped_image, flipped_mask\n",
        "\n",
        "    def flip_vertically_data(self):\n",
        "        \"\"\" Flip Vertically\n",
        "        \"\"\"\n",
        "        if self.vertical_flip == True:\n",
        "            flipped_image = np.flip(self.image, 0)\n",
        "            flipped_mask = np.flip(self.mask, 0)\n",
        "            return flipped_image, flipped_mask\n",
        "\n",
        "    def shear_data(self):\n",
        "        \"\"\" Shear\n",
        "        \"\"\"\n",
        "        trans = AffineTransform(shear = 0.2)\n",
        "        self.image = warp(self.image, inverse_map= trans)\n",
        "        self.mask = warp(self.mask, inverse_map= trans)\n",
        "        return self.image, self.mask\n",
        "\n",
        "    def augment(self):\n",
        "        if self.activate == True:\n",
        "            images = []\n",
        "            masks = []\n",
        "            images.append(self.image)\n",
        "            masks.append(self.mask)\n",
        "            # print('Augmentation:: Image List Size: ',len(images))\n",
        "            if self.rotation != 0:\n",
        "                self.image, self.mask = self.rotate_data()\n",
        "                images.append(self.image)\n",
        "                masks.append(self.mask)\n",
        "\n",
        "            if self.zoom_range != 1:\n",
        "                self.image, self.mask = self.rescale_data()\n",
        "                images.append(self.image)\n",
        "                masks.append(self.mask)\n",
        "\n",
        "            if self.horizontal_flip == True:\n",
        "                self.image, self.mask = self.flip_horizontal_data()\n",
        "                images.append(self.image)\n",
        "                masks.append(self.mask)\n",
        "\n",
        "            if self.vertical_flip == True:\n",
        "                self.image, self.mask = self.flip_vertically_data()\n",
        "                images.append(self.image)\n",
        "                masks.append(self.mask)\n",
        "\n",
        "            if self.shear != 0:\n",
        "                self.image, self.mask = self.shear_data()\n",
        "                images.append(self.image)\n",
        "                masks.append(self.mask)\n",
        "        else:\n",
        "            images = []\n",
        "            masks = []\n",
        "            images.append(self.image)\n",
        "            masks.append(self.mask)\n",
        "\n",
        "        images = np.array(images)\n",
        "        masks = np.array(masks)\n",
        "\n",
        "        return images, masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipPFNYFABIyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import Sequence\n",
        "class InriaDataLoader(Sequence):\n",
        "    \"\"\" Load the training dataset for Inria Dataset from the\n",
        "        data folder and put in an array\n",
        "\n",
        "        Parameters: - data_path: Loads the datapath\n",
        "                    - patch_size: Format the image sizes (default: 256x256)\n",
        "                    - train_ids:\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, data_ids, data_path, patch_size = 256,\n",
        "                batch_size = 8, aug = False, rotation = 0,\n",
        "                zoom_range = 1, horizontal_flip = False,\n",
        "                vertical_flip = False, shear = 0):\n",
        "\n",
        "        self.data_path = data_path\n",
        "        self.data_ids = data_ids\n",
        "        self.patch_size = patch_size\n",
        "        self.batch_size = batch_size\n",
        "        self.aug = aug\n",
        "        self.rotation = rotation\n",
        "        self.zoom_range = zoom_range\n",
        "        self.horizontal_flip = horizontal_flip\n",
        "        self.vertical_flip = vertical_flip\n",
        "        self.shear = shear\n",
        "\n",
        "    def __load__(self, data_name):\n",
        "        \"\"\" Load an image and a mask from the data folder\n",
        "            Parameters: Image name\n",
        "        \"\"\"\n",
        "        image_name_path = os.path.join(self.data_path,'images/', data_name)\n",
        "        mask_name_path = os.path.join(self.data_path, 'gt', data_name)\n",
        "\n",
        "        image = cv2.imread(image_name_path, 1)\n",
        "        image = cv2.resize(image, (self.patch_size,\n",
        "                            self.patch_size))\n",
        "\n",
        "        mask = cv2.imread(mask_name_path)\n",
        "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "        mask = cv2.resize(mask, (self.patch_size,\n",
        "                            self.patch_size))\n",
        "        mask = mask[:, :, np.newaxis]\n",
        "\n",
        "        image = image/255.\n",
        "        \n",
        "        mask = mask/255.\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Get all the images and masks in the data folder\n",
        "            and put into array\n",
        "        \"\"\"\n",
        "        if(index+1)*self.batch_size > len(self.data_ids):\n",
        "            self.batch_size = len(self.data_ids) - index*self.batch_size\n",
        "\n",
        "        files_batch = self.data_ids[index * \\\n",
        "            self.batch_size: (index + 1) * self.batch_size]\n",
        "\n",
        "        images = []\n",
        "        masks = []\n",
        "\n",
        "        for file in files_batch:\n",
        "            image, mask = self.__load__(file)\n",
        "    \n",
        "            aug = DataAugmentation(image, mask,\n",
        "                                   rotation =self.rotation,\n",
        "                                   zoom_range = self.zoom_range,\n",
        "                                   horizontal_flip = self.horizontal_flip,\n",
        "                                   vertical_flip = self.vertical_flip,\n",
        "                                   activate = self.aug)\n",
        "\n",
        "            aug_images, aug_masks = aug.augment()\n",
        "\n",
        "            for aug_image in aug_images:\n",
        "                images.append(aug_image)\n",
        "\n",
        "            for aug_mask in aug_masks:\n",
        "                masks.append(aug_mask)\n",
        "              \n",
        "\n",
        "        images = np.array(images)\n",
        "        masks = np.array(masks)\n",
        "\n",
        "        return images, masks\n",
        "\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        pass\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.data_ids)/float(self.batch_size)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seJU3ptRBK41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Visualizer():\n",
        "    \"\"\" Class for visualizing input image and corresponding\n",
        "        mask\n",
        "\n",
        "        Parameters: image\n",
        "                    mask\n",
        "    \"\"\"\n",
        "    def __init__(self, image, mask, image_size = 256):\n",
        "        self.image = image\n",
        "        self.mask = mask\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def plot(self):\n",
        "        fig = plt.figure()\n",
        "        fig.subplots_adjust(hspace = 0.4, wspace = 0.4)\n",
        "\n",
        "        fig_a = fig.add_subplot(1, 2, 1)\n",
        "        fig_a.set_title('Input Image')\n",
        "        image = np.reshape(self.image[0]*255, \n",
        "                           (self.image_size, self.image_size))\n",
        "        plt.imshow(image)\n",
        "\n",
        "        fig_b = fig.add_subplot(1, 2, 2)\n",
        "        fig_b.set_title('Output Mask')\n",
        "        mask = np.reshape(self.mask[0]*255, \n",
        "                          (self.image_size, self.image_size))\n",
        "        plt.imshow(mask)\n",
        "\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ppkZzv9CgjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Lambda, Activation\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import Add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# def dice_coef(y_true, y_pred):\n",
        "#     return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.)\n",
        "class FCN():\n",
        "    '''\n",
        "    '''\n",
        "    def __init__(self,num_classes = 1, input_shape = (256,256,3), lr_init = 0.0001, lr_decay=0.0005):\n",
        "        self.num_classes = num_classes\n",
        "        self.input_shape = input_shape\n",
        "        self.lr_init = lr_init\n",
        "        self.lr_decay = lr_decay\n",
        "#         self.vgg_weight_path = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "        self.vgg_weight_path = None\n",
        "    \n",
        "    def Convblock(self,channel_dimension, block_no, no_of_convs) :\n",
        "      Layers = []\n",
        "      for i in range(no_of_convs) :\n",
        "\n",
        "          Conv_name = \"conv\"+str(block_no)+\"_\"+str(i+1)\n",
        "\n",
        "          # A constant kernel size of 3*3 is used for all convolutions\n",
        "          Layers.append(Convolution2D(channel_dimension,kernel_size = (3,3),padding = \"same\",activation = \"relu\",name = Conv_name))\n",
        "\n",
        "      Max_pooling_name = \"pool\"+str(block_no)\n",
        "\n",
        "      #Addding max pooling layer\n",
        "      Layers.append(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),name = Max_pooling_name))\n",
        "\n",
        "      return Layers\n",
        "  \n",
        "    def FCN_8_helper(self,image_size=256):\n",
        "      model = Sequential()\n",
        "      model.add(Permute((1,2,3),input_shape = (image_size,image_size,3)))\n",
        "\n",
        "      for l in self.Convblock(64,1,2) :\n",
        "          model.add(l)\n",
        "\n",
        "      for l in self.Convblock(128,2,2):\n",
        "          model.add(l)\n",
        "\n",
        "      for l in self.Convblock(256,3,3):\n",
        "          model.add(l)\n",
        "\n",
        "      for l in self.Convblock(512,4,3):\n",
        "          model.add(l)\n",
        "\n",
        "      for l in self.Convblock(512,5,3):\n",
        "          model.add(l)\n",
        "\n",
        "      model.add(Convolution2D(4096,kernel_size=(7,7),padding = \"same\",activation = \"relu\",name = \"fc6\"))\n",
        "\n",
        "      #Replacing fully connnected layers of VGG Net using convolutions\n",
        "      model.add(Convolution2D(4096,kernel_size=(1,1),padding = \"same\",activation = \"relu\",name = \"fc7\"))\n",
        "\n",
        "      # Gives the classifications scores for each of the 21 classes including background\n",
        "      model.add(Convolution2D(21,kernel_size=(1,1),padding=\"same\",activation=\"relu\",name = \"score_fr\"))\n",
        "\n",
        "      Conv_size = model.layers[-1].output_shape[2] #16 if image size if 512\n",
        "      #print(Conv_size)\n",
        "\n",
        "      model.add(Deconvolution2D(21,kernel_size=(4,4),strides = (2,2),padding = \"valid\",activation=None,name = \"score2\"))\n",
        "\n",
        "      # O = ((I-K+2*P)/Stride)+1 \n",
        "      # O = Output dimesnion after convolution\n",
        "      # I = Input dimnesion\n",
        "      # K = kernel Size\n",
        "      # P = Padding\n",
        "\n",
        "      # I = (O-1)*Stride + K \n",
        "      Deconv_size = model.layers[-1].output_shape[2] #34 if image size is 512*512\n",
        "\n",
        "      #print(Deconv_size)\n",
        "      # 2 if image size is 512*512\n",
        "      Extra = (Deconv_size - 2*Conv_size)\n",
        "\n",
        "      #print(Extra)\n",
        "\n",
        "      #Cropping to get correct size\n",
        "      model.add(Cropping2D(cropping=((0,Extra),(0,Extra))))\n",
        "\n",
        "      return model\n",
        "    \n",
        "    def network(self):\n",
        "      fcn_8 = self.FCN_8_helper(256)\n",
        "      #Calculating conv size after the sequential block\n",
        "      #32 if image size is 512*512\n",
        "      Conv_size = fcn_8.layers[-1].output_shape[2] \n",
        "\n",
        "      #Conv to be applied on Pool4\n",
        "      skip_con1 = Convolution2D(1,kernel_size=(1,1),padding = \"same\",activation=None, name = \"score_pool4\")\n",
        "\n",
        "      #Addig skip connection which takes adds the output of Max pooling layer 4 to current layer\n",
        "      Summed = add(inputs = [skip_con1(fcn_8.layers[14].output),fcn_8.layers[-1].output])\n",
        "\n",
        "      #Upsampling output of first skip connection\n",
        "      x = Deconvolution2D(1,kernel_size=(4,4),strides = (2,2),padding = \"valid\",activation=None,name = \"score4\")(Summed)\n",
        "      x = Cropping2D(cropping=((0,2),(0,2)))(x)\n",
        "\n",
        "\n",
        "      #Conv to be applied to pool3\n",
        "      skip_con2 = Convolution2D(1,kernel_size=(1,1),padding = \"same\",activation=None, name = \"score_pool3\")\n",
        "\n",
        "      #Adding skip connection which takes output og Max pooling layer 3 to current layer\n",
        "      Summed = add(inputs = [skip_con2(fcn_8.layers[10].output),x])\n",
        "\n",
        "      #Final Up convolution which restores the original image size\n",
        "      Up = Deconvolution2D(1,kernel_size=(16,16),strides = (8,8),\n",
        "                           padding = \"valid\",activation = None,name = \"upsample\")(Summed)\n",
        "\n",
        "      #Cropping the extra part obtained due to transpose convolution\n",
        "      final = Cropping2D(cropping = ((0,8),(0,8)))(Up)\n",
        "\n",
        "\n",
        "      return Model(fcn_8.input, final)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqcxwvKNCjA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"\"\"\n",
        "# Name: Train U-Net Model\n",
        "# Author: Arghadeep Mazumder\n",
        "# Version: 0.1\n",
        "# Description: Train U-Net Model\n",
        "\n",
        "# \"\"\"\n",
        "# import os\n",
        "# import sys\n",
        "# import cv2\n",
        "# import keras.utils\n",
        "# import numpy as np\n",
        "# sys.path.append('../')\n",
        "# from utils.iou import IoU\n",
        "# from models.fcn import FCN\n",
        "# from utils.loader import InriaDataLoader\n",
        "# from keras.models import Model\n",
        "# from keras.layers import *\n",
        "# from keras.optimizers import *\n",
        "# from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "# from keras import backend as keras\n",
        "# from utils.visualizer import InriaVisualizer\n",
        "\n",
        "class TrainFCN():\n",
        "    \"\"\" Class for training U-Net Model\n",
        "\n",
        "        Parameters: train_path       = train folder\n",
        "                    patch_size      = 256\n",
        "                    activate_aug    = False\n",
        "                    rotation        = 0\n",
        "                    zoom_range      = 1\n",
        "                    horizontal_flip = False\n",
        "                    vertical_flip   = False\n",
        "                    shear           = 0\n",
        "                    net             = U-Net\n",
        "                    epochs          = 5\n",
        "                    batch_size      = 1\n",
        "                    learning_rate   = 0.2\n",
        "                    val_percent     =\n",
        "                    save_model      = False\n",
        "                    activate_gpu    = False\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                train_path,\n",
        "                image_size = 256,\n",
        "                activate_aug = False,\n",
        "                rotation = 0,\n",
        "                zoom_range = 1,\n",
        "                horizontal_flip = False,\n",
        "                vertical_flip = False,\n",
        "                shear = 0,\n",
        "                epochs = 10,\n",
        "                batch_size = 8,\n",
        "                learning_rate = 0.1,\n",
        "                save_model = True,\n",
        "                val_data_size = 10,\n",
        "                evaluate = True):\n",
        "\n",
        "        self.train_path = train_path\n",
        "        self.image_size = image_size\n",
        "        self.activate_aug = activate_aug\n",
        "        self.rotation = rotation\n",
        "        self.zoom_range = zoom_range\n",
        "        self.horizontal_flip = horizontal_flip\n",
        "        self.vertical_flip = vertical_flip\n",
        "        self.shear = shear\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.save_model = save_model\n",
        "        self.val_data_size = val_data_size\n",
        "        self.evaluate = evaluate\n",
        "\n",
        "    def train(self):\n",
        "        print('''\n",
        "Start Training:\n",
        "\n",
        "    Data Path:       {}\n",
        "    Model:           {}\n",
        "    Patch Size:      {}\n",
        "    Augmentation:    {}\n",
        "    Rotation:        {}\n",
        "    Zoom Range:      {}\n",
        "    Horizontal Flip: {}\n",
        "    Vertical Flip:   {}\n",
        "    Shear:           {}\n",
        "    Epochs:          {}\n",
        "    Batch Size:      {}\n",
        "    Learning Rate:   {}\n",
        "    Validation %:    {}\n",
        "    Save Model:      {}\n",
        "    Evaluate:        {}\n",
        "\n",
        "        '''.format(str(self.train_path), str('U-Net'), self.image_size,\n",
        "            self.activate_aug, self.rotation, self.zoom_range,\n",
        "            self.horizontal_flip, self.vertical_flip, self.shear,\n",
        "            self.epochs, self.batch_size, self.learning_rate,\n",
        "            self.val_data_size, self.save_model, self.evaluate))\n",
        "\n",
        "        images_path = os.path.join(self.train_path, 'images/')\n",
        "        train_ids = next(os.walk(images_path))[2]\n",
        "\n",
        "        valid_ids = train_ids[:self.val_data_size]\n",
        "        train_ids = train_ids[self.val_data_size:]\n",
        "\n",
        "\n",
        "        models = FCN()\n",
        "        model = models.network()\n",
        "        model.compile(optimizer = 'adam',\n",
        "                    loss = 'binary_crossentropy',\n",
        "                    metrics = ['acc'])\n",
        "        model.summary()\n",
        "\n",
        "        train_gen = InriaDataLoader(train_ids,\n",
        "                                    self.train_path,\n",
        "                                    patch_size = self.image_size,\n",
        "                                    batch_size = self.batch_size)\n",
        "\n",
        "        valid_gen = InriaDataLoader(valid_ids,\n",
        "                                    self.train_path,\n",
        "                                    patch_size = self.image_size,\n",
        "                                    batch_size = self.batch_size)\n",
        "\n",
        "        train_steps = len(train_ids) // self.batch_size\n",
        "        valid_steps = len(valid_ids) // self.batch_size\n",
        "        print(train_steps)\n",
        "        model.fit_generator(train_gen,\n",
        "                            validation_data = valid_gen,\n",
        "                            steps_per_epoch = train_steps,\n",
        "                            validation_steps = valid_steps,\n",
        "                            epochs = self.epochs)\n",
        "        if self.save_model:\n",
        "            model.save('building_unet.h5')\n",
        "\n",
        "#         keras.utils.plot_model(model, to_file = 'unet_architecture.png')\n",
        "\n",
        "        if self.evaluate:\n",
        "            image , mask = valid_gen.__getitem__(2)\n",
        "            result = model.predict(image)\n",
        "            result = result > 0.5\n",
        "            viz = Visualizer(mask, result)\n",
        "            viz.plot()\n",
        "\n",
        "#         iou_score = IoU(target= mask, prediction=result)\n",
        "#         print('IoU Score: ',iou_score)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSFQ_l-bC0tT",
        "colab_type": "code",
        "outputId": "44719286-b1f9-4ae3-cec8-9567c63f154e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_fcn = TrainFCN(train_path='dummy_Inria_data/train')\n",
        "train_fcn.train()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start Training:\n",
            "\n",
            "    Data Path:       dummy_Inria_data/train\n",
            "    Model:           U-Net\n",
            "    Patch Size:      256\n",
            "    Augmentation:    False\n",
            "    Rotation:        0\n",
            "    Zoom Range:      1\n",
            "    Horizontal Flip: False\n",
            "    Vertical Flip:   False\n",
            "    Shear:           0\n",
            "    Epochs:          10\n",
            "    Batch Size:      8\n",
            "    Learning Rate:   0.1\n",
            "    Validation %:    10\n",
            "    Save Model:      True\n",
            "    Evaluate:        True\n",
            "\n",
            "        \n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "permute_3_input (InputLayer)    (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "permute_3 (Permute)             (None, 256, 256, 3)  0           permute_3_input[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_1 (Conv2D)                (None, 256, 256, 64) 1792        permute_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_2 (Conv2D)                (None, 256, 256, 64) 36928       conv1_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 128, 128, 64) 0           conv1_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1 (Conv2D)                (None, 128, 128, 128 73856       pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2 (Conv2D)                (None, 128, 128, 128 147584      conv2_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pool2 (MaxPooling2D)            (None, 64, 64, 128)  0           conv2_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1 (Conv2D)                (None, 64, 64, 256)  295168      pool2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2 (Conv2D)                (None, 64, 64, 256)  590080      conv3_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3 (Conv2D)                (None, 64, 64, 256)  590080      conv3_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pool3 (MaxPooling2D)            (None, 32, 32, 256)  0           conv3_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1 (Conv2D)                (None, 32, 32, 512)  1180160     pool3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2 (Conv2D)                (None, 32, 32, 512)  2359808     conv4_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3 (Conv2D)                (None, 32, 32, 512)  2359808     conv4_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pool4 (MaxPooling2D)            (None, 16, 16, 512)  0           conv4_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1 (Conv2D)                (None, 16, 16, 512)  2359808     pool4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2 (Conv2D)                (None, 16, 16, 512)  2359808     conv5_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3 (Conv2D)                (None, 16, 16, 512)  2359808     conv5_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "pool5 (MaxPooling2D)            (None, 8, 8, 512)    0           conv5_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "fc6 (Conv2D)                    (None, 8, 8, 4096)   102764544   pool5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "fc7 (Conv2D)                    (None, 8, 8, 4096)   16781312    fc6[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "score_fr (Conv2D)               (None, 8, 8, 21)     86037       fc7[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "score2 (Conv2DTranspose)        (None, 18, 18, 21)   7077        score_fr[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "score_pool4 (Conv2D)            (None, 16, 16, 1)    513         pool4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_7 (Cropping2D)       (None, 16, 16, 21)   0           score2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 21)   0           score_pool4[0][0]                \n",
            "                                                                 cropping2d_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "score4 (Conv2DTranspose)        (None, 34, 34, 1)    337         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "score_pool3 (Conv2D)            (None, 32, 32, 1)    257         pool3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_8 (Cropping2D)       (None, 32, 32, 1)    0           score4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 32, 32, 1)    0           score_pool3[0][0]                \n",
            "                                                                 cropping2d_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "upsample (Conv2DTranspose)      (None, 264, 264, 1)  257         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_9 (Cropping2D)       (None, 256, 256, 1)  0           upsample[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 134,355,022\n",
            "Trainable params: 134,355,022\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "123\n",
            "Epoch 1/10\n",
            "123/123 [==============================] - 116s 946ms/step - loss: 0.6611 - acc: 0.8392 - val_loss: 0.6518 - val_acc: 0.7344\n",
            "Epoch 2/10\n",
            "123/123 [==============================] - 107s 866ms/step - loss: 0.4150 - acc: 0.8410 - val_loss: 0.7732 - val_acc: 0.6884\n",
            "Epoch 3/10\n",
            "123/123 [==============================] - 106s 865ms/step - loss: 0.3841 - acc: 0.8439 - val_loss: 0.8323 - val_acc: 0.6719\n",
            "Epoch 4/10\n",
            "123/123 [==============================] - 106s 865ms/step - loss: 0.3599 - acc: 0.8514 - val_loss: 0.7319 - val_acc: 0.6611\n",
            "Epoch 5/10\n",
            "123/123 [==============================] - 106s 864ms/step - loss: 0.3226 - acc: 0.8643 - val_loss: 0.4605 - val_acc: 0.7759\n",
            "Epoch 6/10\n",
            "123/123 [==============================] - 107s 866ms/step - loss: 0.3614 - acc: 0.8524 - val_loss: 0.7589 - val_acc: 0.6353\n",
            "Epoch 7/10\n",
            "123/123 [==============================] - 106s 864ms/step - loss: 0.3568 - acc: 0.8528 - val_loss: 0.7327 - val_acc: 0.6714\n",
            "Epoch 8/10\n",
            "123/123 [==============================] - 107s 867ms/step - loss: 0.3485 - acc: 0.8573 - val_loss: 0.6540 - val_acc: 0.6897\n",
            "Epoch 9/10\n",
            "123/123 [==============================] - 106s 865ms/step - loss: 0.3216 - acc: 0.8671 - val_loss: 0.5979 - val_acc: 0.7318\n",
            "Epoch 10/10\n",
            "123/123 [==============================] - 107s 868ms/step - loss: 0.2996 - acc: 0.8747 - val_loss: 0.6748 - val_acc: 0.7119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAC6CAYAAACgP4aQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHj5JREFUeJzt3XmcVmX9//HXm2EAERUQIgZRkCVD\nzckGQXPrqzlCC5p9Scvk67fCxCzbqSxt+bXb4ldBTcp9QUzFXEalqFxAUAcQSEB2BkFFEEQRhs/v\nj/vcdjPNcN8z932fbT7Px2Mec+Y65z7X58yc6zPXuc4mM8M551x6dYg6AOecc+Xlid4551LOE71z\nzqWcJ3rnnEs5T/TOOZdynuidcy7lPNE751wekgZIMkkdo46lLTzRNyFppaRTQ6jnckm3xCEW55qS\n9D+SFkjaLuklSZMldW/F50u67+Zbn6STg0R8T5Pyo4LymaWKJYk80Tvn9iDp68AvgG8CBwAjgUOA\nRyV1ijK2PF4GjpV0YE7ZOGBJRPHEhif6vQh6NY9L+rWk1yStkDQqZ/5MST+T9LSk1yXdJ6lnMO9k\nSWubrG+lpFMlnQ58F/iUpG2S5hUYyxOSfitps6Tlko4LytdI2ihpXM7yH5H0XBDXGkmXN1nfeZJW\nSXpV0vdze0ySOkiaKOnFYP7U7Ha5dJO0P/BD4GIze9jMdprZSmAsMAA4N1juBkk/yfncO/u7pJuB\ng4H7g/37WzlDH+MlNUhaL+kbOZ9v1fpaCP9t4F7g7OBzFcCngFubbOPvgzbxuqRnJJ2QM+8YSXOD\neRsk/aaF39NZQZs5Iv9vNXqe6PMbAbwA9AJ+CUyRpJz55wH/C/QFdgFX5luhmT0M/BS408y6mdlR\nrYhlPnAgcBtwBzAcGEymAV4lqVuw7BtBbN2BjwAXSjoDQNIwYBLwmSDuA4B+OfVcDJwBnARUAa8B\nVxcYo0u244AuwJ9zC81sG/Ag8OF8KzCzzwKrgY8F+/cvc2Z/CBgCnAZ8u5DhnTzra+omMvs9QC3w\nPNDQZJk5QDXQk0w7uktSl2De74Hfm9n+wCBgatMKJJ1P5ojnVDN7Pl/8ceCJPr9VZvYHM2sEbiST\nGPvkzL/ZzJ43szeA7wNjg55EOawwsz8FsdwJ9Ad+ZGY7zOwRMj2awQBmNtPMFpjZbjObD9xOJnED\nfBK438weN7O3gR8AuQ89+iLwPTNba2Y7gMuBTyb1RJRrlV7AK2a2q5l564P5xfihmb1hZguAPwHn\nFLm+PZjZk0BPSe8hk/BvamaZW8zsVTPbZWZXAJ2B9wSzdwKDJfUys21mNqvJxy8hM6R1spktK2Xs\n5eSJPr+XshNmtj2Y7JYzf03O9CqgkuIbQ0s25Ey/GcTUtKwbgKQRkv4m6WVJW8gk72xcVblxB9v1\nas56DgHuCYaINgOLgUb2/Afn0ukVoFcL/9T7BvOL0bS9VBW5vubcDHyJzNHDPU1nSvqGpMWStgT7\n9wH8u218DhgK/EvSHEkfbfLxbwJXm9laEsQTffH650wfTKZH8AqZoZOu2RlBL793zrLlfmzobcB0\noL+ZHQBcA2SHnNYDB+XEtg+Z4aCsNcAoM+ue89XFzNaVOWYXvaeAHcAncguDIcFRwIygaI/9G3h3\nk/W0tH83bS/ZYZW2rq85NwMTgAdzOmcABOPx3yJzzqGHmXUHthC0DTNbambnAO8iMzwzTdK+Oas4\nDbhU0lmtiCdynuiLd66kYZK6Aj8CpgVDK0uALsFJ0UrgUjKHiFkbgAGSyvU32A/YZGZvSToG+HTO\nvGnAx4KTuZ3IDM3knne4Bvh/kg4BkNRb0pgyxelixMy2kDkZ+3+STpdUKWkAmbHqtWSSKEA9MFpS\nT0nvJjOkkWsDcGgzVXxfUldJhwPnkxmCLGZ9zW3DCjLDlN9rZvZ+ZM6lvQx0lPQDYP/sTEnnSupt\nZruBzUHx7pzPLwROB66W9PFC4okDT/TFuxm4gcwQTxfgy/BOg5kAXA+sI9NjyT3cuyv4/qqkZ8sQ\n1wTgR5K2khmDf+ekkpktJHPC9Q4yvfttwEYyPTnInJCaDjwSfH4WmRPBrh0ITnZ+F/g18Dowm8xR\n3inBORvI7PfzgJXAI/w7YWf9jEzPd3Pu1TXA34FlZI4Mfh2cWypmfS1tw+Nm1vQkLEAd8DCZjtgq\n4C32HE46HVgoaRuZdnC2mb3ZZN3zgI8Cf1DOVXhxJn/xSNspcxPGLWZ2fdSxFCM4LN8MDAl6Q86V\nVHBUsAKobOFErysj79G3U5I+FhxC70um57aATG/KOZcyZUv0wfjeC5KWSZpYrnpcm40hcyKsgcx1\nzWebH96VhbcFF7WyDN0EV5gsIXNzxVoyNyicY2aLSl6ZczHmbcHFQbl69McAy8xseXBDzh1kepDO\ntTfeFlzkynWnYz/2PJO9lr1ctdFJna0L+7Y0OzGGvm97i/OWzO/a4jzXOlt57RUz651/yVhoVVuA\n9LQHV35v8QZv2w7lWy6yW9oljQfGA3ShKyN0SlShlM4CqOjdmwfnPbpH8bBJExix4MmIgkqfx2za\nqqhjKLVUtgdXdrNtRv6FKF+iX8eed8AdFJS9w8yuA64D2F89U3MSsPHll6mtqqZjvyp23dSBuvf+\nhQFXLaQx6sBcVPK2BUhve3DxUK5EPwcYImkgmZ36bPa8MzP1dq1rgFOglmoyd1i7dqrdtwUXvbKc\njA1uiPgSmbvQFgNTg7sxE2Xr2SOpa6hneH0jq6YeGXU4LoHS0hZcspVtjN7MHiTz/OrE+swPHgDg\nJ+9awE/etYDVq7dxWcMoGkZujTgylyRpaAsu2fzO2L24qPuaPX4+uGM3/nTwP9nvn+V6CrFzzpWe\nJ/oWNHzruBbnTRv0GHUN9ex4ZABv1g0MMSrnnGs9f2NQCxZcMinvMjOPuDcz0QBHzv40VWf6zY7O\nufjxRN+M1ZcfR+bx2IVbMOI2aIBBfz2fwec+V57AnIuhuoY920ptVfU75dlpFy0fumnG4vH5e/Mt\nefG//kRdQz2vPzSI3cf7Tu7Sqa6h/p2v5uY1N+2i44m+iRd/dWxJ1vPUUXfz6NQb2PHIAHaeVlOS\ndToXB4Ukb0/w8eKJPsebY46heuTSkq5z5hH3UvHtDewYNbyk63UuKj4ckzw+Rp+j+9dWM23QYyVf\n76PvvR+mZKaHTZ5A/x/7c29cMnlPPZm8Rx/YeVoN04c8XPZ6Fl04ibqGetZ/reXLN51LG/8HES1P\n9IFOE9eHWt/8b0zya/Bdu+HDPdHyRA/YcUfx8GEPhF7vPrX+Hm7nXPl5ogfsx5tCr/PI2f4AQ+dc\nODzRE5wsDdn2FfuHXqdzxfIhmGRq94n+5envCb3O+W+/xeCvzQq9Xpd+Ld3EVMr1F5Ls/R9CvLT7\nRP9szZ2h13nG/V8JvU6XTtmknk3wYSTY1tww5Qk/Htp1oq84sGfodW5sfIMhF88OvV6XTk0TqV/G\n6JrTrm+Y2n3XPqHXOeL+rzKUp0Ov16VLtvee23NumuRzlylVz7q59eSu3x9kFk/tukcfxSWVQyd4\nknfFa5rYm0vyLS1bCuVevyutdpvoK2f2Db3OQ+++IPQ6XToVmljLNVbedH3Zn703H09FDd1IWgls\nBRqBXWZWI6kncCcwAFgJjDWz14oLs8RmHMStg+8Cwhu6OfTuCzjs+/+iMbQaXdgS2x5c6pWiR/8h\nM6s2s+yzeCcCM8xsCDAj+DlWbh16Jwd0CHd8vuMbHWjcvCXUOl0kYtUevIftoDxDN2OAG4PpG4Ez\nylBHm215cDC9KvYNvd6BE58Kvc5UOObIqCMoVizaQ2vG0Pc23u+SqdhEb8Ajkp6RND4o62Nm2SeE\nvQT0ae6DksZLmitp7k52FBlG4aYdcUNodWUNmzwh9DrTou7em6lrqGfQnC5Rh1KI2LaH1vbsc2+8\n8qOC5Cs20R9vZkcDo4CLJJ2YO9PMjMzO/x/M7DozqzGzmko6FxlG4Q7q2C20urIG3Lkh9DrTYPcJ\n739nelK/WfR+snuE0RQkce2hqaaXR5b7TlsXjqJOxprZuuD7Rkn3AMcAGyT1NbP1kvoCG0sQZ0mc\n98Ka0Osc+PDnGbpkbuj1Jt22sSN54nfX7FF2y4CZ0AAV4V8wVZA4tYe29sKbXj2Te+dtMet10Wpz\nope0L9DBzLYG06cBPwKmA+OAnwff7ytFoKXwmf1eDb3Ow363jd2h15p8TZN83CWxPWQ1veEpV0vX\nyO9tOf9nED/F9Oj7APdIyq7nNjN7WNIcYKqkzwGrgLHFh1m8Hyx/lrBvGxg8838YNN8Pe1tr0/8e\nCyTu95ao9pBrbzc8tXS3bb7lXLy0OdGb2XLgqGbKXwVOKSaocvhgl/DvDRv8q53ND8i6vZrzk8lR\nh9BqSWsPxciX1P0xCPHTLu6MnbL68dDrPHL2p7HnFoZeb9LteGRA1CG0O9nE3Jbk3NxnPMnHT7tI\n9Pt1qAi9zm53+YtF2mLmEfdGHUIi7C2ZFppoc6+oKeXVNT6MEz/pf3qlRAUKtcrjv3wB+0/zF4u0\n1trvHEcCx+YjVeqnU7ZWc0nde/Tx0y569N06hHuzzb7T/HnzbbHw4klRh5Ao/iAxV6hUJ/oOXbtS\nt+65UOv80PmfD7W+tNh432FRh+BcaqU60T+07MnQ6+xU5zdHtZZqjuC54XdEHYZrAz+aSIbUJvqO\n/apCr/O0s8aFXmfSdXjfYaz5jl+EmlR+KWUypDbRbzrp4NDrrNga3sPZ0uKFCw5g4bG3Rh1Gu1VI\nks73khG/yib+UpnoO3TpwlO/DvcW+qs392f38/8Ktc6kqxhyKMvPvDbqMNq1Qnrke7s7Nnv9vZ8Y\njrdUXl65/cPvA8K9vPGBMcOB5aHWmXSrfxH+y9ndv+Um5eYu02ztpZOe5OMrdYlelZ34+7XXhV5v\n41JP8q1R0etAnh/pQzZx0tIQjUu+1A3d7Dwx/DcSfWT46NDrTLo1f2j2/RvOuTJIXaKfcfOU0Ovc\nta4h9DqTbsGI26IOwbl2I1WJfuepH4g6BFeAl6e/J+oQnGtXUpPo3z59ONdMuTL0ekcNPi70OpPu\n2Zo7ow4hUfzyRVes1CT6Tg/P4eJDPsjoU8N7r0Ntv/eze/v20OpLg8qZMX0PYEz5DUmuFFKT6LMa\nFy2htqq67Heprt21Dczv6GyNHk/05C9DH4o6DOfandRdXpmlp+a90xN6u7aGv/3p+pKu/3MHH1/S\n9aXdqqlHUjfw5qjDSBzvzbtSyNujl/RHSRslPZ9T1lPSo5KWBt97BOWSdKWkZZLmSzq6nMEXqlPd\nXGqrqjn+yxeUZH1PvOWv+26tB0Yk7/WAzYmiPfgYvStWIUM3NwCnNymbCMwwsyHAjOBngFHAkOBr\nPBCr1r3vtNkc+40vFr2eb3/zwhJE074MquwWdQilcgMhtYdsgvdevStW3kRvZv8ANjUpHgPcGEzf\nCJyRU36TZcwCukuK1dm3/W+bRW1VNSdOGM+lG1t/c9UTb+1m37v9xSKtMWrh5qhDKJkw20P2sQTO\nFautJ2P7mNn6YPolIHubYz9gTc5ya4Oy/yBpvKS5kubuJPynPu5z79PMqa7gxAnjOX/1CQV/7is/\nvaiMUaXTJT1WRh1CuZWtPXhv3pVC0VfdmJkBrb78xMyuM7MaM6uppHOxYbTZPvc+TcPIrZx0wXg+\nvrTpEfmeHtleyYHXPxVSZOkxbPKEqEMITdLbg0untib6DdlD0OD7xqB8HdA/Z7mDgrLY63L/0+w4\n6SVqq6oZctOFnLjgzP9Y5huTvxBBZMnX/8dPUltVza1bD4w6lHJJXXtw6dLWRD8dyF6oPg64L6f8\nvOBqg5HAlpxD2sQ4dOJTdPp5D46dd9Ye5X2vCP/VhGnyq6s/xaA7ij8ZHkOpbg8u+WR5bvqRdDtw\nMtAL2ABcBtwLTAUOBlYBY81skyQBV5G5KmE7cL6Z5X2J6v7qaSN0ShGbUUYj38emS99ieJ/VvDj8\nraijSY0l1xzDio+37XHSFX2XPWNmNSUOqSDtvj24WJltM3jdNinfcnkTfRh8x26/Rs7byQ97L2zV\nZ6JM9GHw9uAKVWiiT90jEFyyzDqqktqqao6c/emoQ3EutTzRu1ioOnMRtVXVbbq3wTm3d57oXazM\nqa7g5C98gctePjzqUJxLDU/0LnY6PzCHx782slU3sjnnWuaJ3sVSxxnP0DByK7VV1fzutQFRh+Nc\nosUj0XftgmqO8Od6uGY9dHh3PnLcx7nh9XdFHYpziRSLRD908CYenn4LAKrsFHE0Lo52rVzN7YdV\n+bNfnGuDWCT6PRw1NOoInHMuVWKX6LM9e+ecc6URu0TvnHOutGKZ6Hef9P6oQ3AuFvwCBVcK8Uz0\nFbEMyznnEimWGXXGLVOiDsG5WPCrjFwpxDLRAzR+6OioQ3DOuVSIbaJ/+4COUYfgXOR8jN6VQmwT\n/fGXzfJevXPOlUBsE/1P+8znzV6VUYfhXKR8jN6VQmwTPcATv7826hCccy7x8iZ6SX+UtFHS8zll\nl0taJ6k++BqdM+87kpZJekFSbbkCdy4KYbYHH593pVLIGc8byLzg+KYm5b81s1/nFkgaBpwNHA5U\nAY9JGmpmjYUGNHD6eDq9WsGA7z1V6EecC9MNhNQeaquqqWuo9+EbV7S8id7M/iFpQIHrGwPcYWY7\ngBWSlgHHAHvN2gte682gO7/I4K/OYihPF1iVc+ELoz3k8iTvSqGYMfovSZofHMr2CMr6AWtyllkb\nlP0HSeMlzZU0t8OaTQz+6qwiQnEuciVrDzvZUe5YXTvT1kQ/GRgEVAPrgStauwIzu87MasysppLO\nbQzDuVjw9uBirU2J3sw2mFmjme0G/kDmcBRgHdA/Z9GDgjLnUquc7cFPyLpSaFOil9Q358czgewV\nCNOBsyV1ljQQGAI+6O7SzduDi7u8J2Ml3Q6cDPSStBa4DDhZUjVgwErgAgAzWyhpKrAI2AVc1Jor\nbpyLu7Dbg5+MdaUgM4s6BvZXTxuhU6IOwyXEYzbtGTOriTqOcvH24Ao122bwum1SvuVifWesc865\n4nmidy7G/GSsKwVP9M7FmI/Ru1LwRO+ccynnid4551LOE71zzqWcJ3rnnEs5T/TOOZdynuidcy5C\nYVxC64neOeciVu5k74neOedioJzJ3hO9c85FoK6hPrRXRRbyzljnnHNlkE3y5U723qN3Lqb8OTfp\nFubjLTzROxdTtVXVnuxdSXiidy7G/KFmrhQ80TvnXMp5oncu5nz4xhUrb6KX1F/S3yQtkrRQ0leC\n8p6SHpW0NPjeIyiXpCslLZM0X9LR5d4I58IQRVsI6/I7l26F9Oh3AV83s2HASOAiScOAicAMMxsC\nzAh+BhhF5m33Q4DxwOSSR+1cNLwtuETKm+jNbL2ZPRtMbwUWA/2AMcCNwWI3AmcE02OAmyxjFtBd\nUt+SR+6KtnHCcVGHkChRtAXvzbtSaNUYvaQBwPuB2UAfM1sfzHoJ6BNM9wPW5HxsbVDmYqJD167U\nNdTz3KWTqGuoZ/N5x0YdUuJ4W3BJUnCil9QNuBu4xMxez51nZgZYayqWNF7SXElzd7KjNR91RXpo\n2ZN7/Dz755Opa6hn+ydGUNHnXRFFlRylbgvBOpttD34i1pVCQYleUiWZHftWM/tzULwhexgafN8Y\nlK8D+ud8/KCgbA9mdp2Z1ZhZTSWd2xq/ayVVdmpx3j+vupaV4wfTse+7Q4woWcrRFsDbgyuvQq66\nETAFWGxmv8mZNR0YF0yPA+7LKT8vuOJgJLAl57DWRWz76L2P+S66cBIPPPMwK352rCf8JqJoCz5G\n70qhkIeafRD4LLBAUvY48rvAz4Gpkj4HrALGBvMeBEYDy4DtwPkljdi1mSo78Y/J1xW07JJxk2Ec\njBp1DrvnLS5zZInhbcElUt5Eb2aPA2ph9inNLG/ARUXG5cpg5wlHAk+36jMPPXQ7AAMf/Dzv/fZy\nGl/dVIbIkiHstuDX0LtS8Ttj24sOFcy4ZUqbP75i9PU8uOCvrJl2BB26di1hYK4l/lAzVyqe6NsJ\nG3FESdaz6LhbeGjZk+ye0T//wq5o3qN3peCJvp24665rSrq+Bw+7l33/0buk63TOlUcsEv3Q923n\nFytmRx1Gau36rw9wQId9SrrOSlXw58GPUtdQ738752IuFokeoLpzZyqGHBp1GKmz89QPcNn1bR+b\nL0R1587UNdRz4dJlZa3HOdc2sUn0AI1Ll0cdQup8cdI0TuwSTl1n7LuNuoZ6Ri3czKiFm8Op1DmX\nV2wS/eijPhx1CKmzY9RwxnbbEnq9l/RYySU9VlLXUM8Rz8RmF3Ou3YpNK2x8+eWoQ0idU3/xz6hD\n4Iq+z1LXUE+PJ3pGHYpz7VYsEv3itX71Rjlc2utfUYfwjjsG/pXKmX3ZdcoHog7FuXYnFom+YtMb\nUYeQOq+Mj9+jh/8y9CFm3DyF98ytZPsnRkQdjnPtRiwSvSu9Zy6P78uMrqyawz+vupb3PtORLeeO\njDoc51LPE30KvfTVZLw56nd95zLrl9dQ11DP0ht8SMe5cvFEn0Lzvjkp6hBabflpUzIJ/2of0nGu\n1DzRp8zbtTVRh1CU5Wdey7Lf+nCOc6XkiT5l7OuvRB1C0V78VGY45/wXVrHhy8kYhnIuzjzRp8ya\njT2iDqFkzt7vNeonTuKsxRvzL+yca1Ehb5hyCTL43OeopZqGe4axYMRtUYdTEne/119YDnu+iKSl\n59T7Y43jo7m/UVR/H+/Rp1TVmYuorarmxAVncv7qE6IOp80u3Xhk1CHEQjZp5HsRSV1Dvb+sJCbi\n9E/XE33K7VO7goaRWxl96lguWBu/m6jymVNdEXUIsZCbNPaWyGurqvP2+l04mv7+o0z8eRO9pP6S\n/iZpkaSFkr4SlF8uaZ2k+uBrdM5nviNpmaQXJNWWcwNcYRoXLWHt2N58aV1yLl9c/Pb2qEPYQ1Rt\nIdtLLyRRZJf1JB8/Uf5dCunR7wK+bmbDgJHARZKGBfN+a2bVwdeDAMG8s4HDgdOBSZK8WxYDu1au\nZunwHYw6dCSXbjySndYYdUgtenHnNsY8dWHUYTQVaVvIJvtCe4b+ztn4iepvkvdkrJmtB9YH01sl\nLQb67eUjY4A7zGwHsELSMuAY4KkSxOtKYPdbbzGnuoKP8gGGzOnMz/v+nW4dQnpofYE++vSFDDxn\nXtRh7CGqttBcYm86lNPSz3EaJ25vWvrdR/E3adUYvaQBwPuB7LvjviRpvqQ/Sspe19cPWJPzsbU0\n0xgkjZc0V9LcnexodeCuNJYO38FZB43kky+eypbdb0YdDgAbG9/g4P9eEHUYe1XKthCsr83toWni\n8OTumio40UvqBtwNXGJmrwOTgUFANZlezhWtqdjMrjOzGjOrqaRzaz7qymDrCa8w9qBjqa2qZsnO\naJ8m+uFnPh9p/fmUui2AtwdXXgUlekmVZHbsW83szwBmtsHMGs1sN/AHMoekAOuA/jkfPygocwlx\n8SEfjLRX+O4zFkdWdz7eFlwSycz2voAk4EZgk5ldklPeNxizRNJXgRFmdrakw4HbyOzsVcAMYIhZ\ny2f+JG0FXih2Y2KqF5D85xL8pyi36xAzC/1tNWG0hWAdaW0PaW0LEN22FdQWCrkz9oPAZ4EFkrKn\ni78LnCOpGjBgJXABgJktlDQVWETmKoWL8u3YwAtmluyncbVA0tw0bltatyuPMNoCpLQ9pHmfifu2\n5e3RhxJEzH9JxUjrtqV1u+Igrb/btG4XxH/b/M5Y55xLubgk+uuiDqCM0rptad2uOEjr7zat2wUx\n37ZYDN0455wrn7j06J1zzpWJJ3rnnEu5yBO9pNODJ/stkzQx6nhaK7jlfaOk53PKekp6VNLS4HuP\noFySrgy2db6ko6OLfO/28qTGxG9bXHlbiKdUtAUzi+wLqABeBA4FOgHzgGFRxtSGbTgROBp4Pqfs\nl8DEYHoi8ItgejTwECAyTz+cHXX8e9muvsDRwfR+wBJgWBq2LY5f3hbiu7+koS1E3aM/BlhmZsvN\n7G3gDjJP/EsMM/sHsKlJ8Rgyd1ASfD8jp/wmy5gFdJfUN5xIW8fM1pvZs8H0ViD7pMbEb1tMeVuI\n6f6ShrYQdaIv+Ol+CdPHglvigZeAPsF0Ire3yZMaU7VtMZLW31+q9pektoWoE33qWeZYLrHXsDbz\npMZ3JH3bXLiSvr8kuS1EnejT+nS/DdlDteD7xqA8Udvb3JMaScm2xVBaf3+p2F+S3haiTvRzgCGS\nBkrqROa1a9MjjqkUpgPjgulxwH055ecFZ+VHAltyDv1iJXhS4xRgsZn9JmdW4rctprwtxHR/SUVb\niPpsMJkz1EvIXHHwvajjaUP8t5N52cROMmNxnwMOJPNI2qXAY0DPYFkBVwfbugCoiTr+vWzX8WQO\nRecD9cHX6DRsW1y/vC1Evw0tbFfi24I/AsE551Iu6qEb55xzZeaJ3jnnUs4TvXPOpZwneuecSzlP\n9M45l3Ke6J1zLuU80TvnXMr9f8gcwbU73tJqAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STDvKGYwnZ3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}